<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | ECE, Virginia Tech | Fall 2015: ECE 5554/4984</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Zoom Background</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Anna Dingler, Henna Mian, Allison Nakazawa, Willem Taylor, and Jessica Tepper</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2020 CS 4476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Institute of Technology</span>
<hr>

<!-- Introduction -->
<h3>Introduction</h3>
<h4>Problem Statement</h4>
The goal of our project is to remove distracting items from a scene in order to use the scene as a Zoom background in meetings. We will use segmentation processes to identify the distracting objects as blobs, seam carving algorithms to remove the blobs, and image quilting algorithms to replace the blob with averaged background pixels. A user can input an image and our system will output options of blobs to be removed. After the user chooses the blobs to remove, our system outputs a final image which can be used as a clean background. The motivation behind our idea comes from the struggle of having to find a blank wall or clean room whenever we need to join a Zoom meeting. With remote interviews and important meetings, our backgrounds become important in how we present ourselves. The system we are presenting would solve that problem by providing a quickly and easily accessible background that's not distracting (ex: textbooks or clothes behind you or preset ocean background). The domain we are working in would be regular RGB photographs.


<br><br>
<!-- Approach -->
<h3>Approach</h3>
Describe very clearly and systematically your approach to solve the problem. Tell us exactly what existing implementations you used to build your system. Tell us what obstacles you faced and how you addressed them. Justify any design choices or judgment calls you made in your approach.

<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
<h4>Existing code and improvements</h4>
To complete this project we will make use of the existing segmentation, seam carving, and image quilting algorithms presented in lecture. We will need to implement the code to perform these algorithms in succession as well as make adjustments to to optimize the performance.
<br><br>We plan to try a few different approaches for the initial segmentation of the picture. The main approaches we plan to explore are K-Means, Mean-Shift, and Graph Cut. Existing implementations for these segmentation approaches can be found here:
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">K-Means</a>,
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html">Mean-Shift</a>, and
<a href="https://github.com/marktao99/python/blob/master/CVP/samples/graphcut.py">Graph Cut</a>.

<br><br>After segmenting the image and allowing the user to select an object to remove, we intend to utilize seam-carving to insert pixels into the areas previously occupied by the removed object and image quilting to match the texture of the surrounding area. Existing implementations can be found here:
<a href="https://github.com/andrewdcampbell/seam-carving">Seam-carving</a> and
<a href="https://github.com/rohitrango/Image-Quilting-for-Texture-Synthesis">Image Quilting</a>.

<br><br>
<h4>Experimental procedure</h4>
<ol>
    <li>Take pictures of scenes with various objects to potentially remove</li>
    <li>Take pictures of the same scenes with the objects physically removed</li>
    <li>Run our segmentation program, highlighting the blobs detected in the image and allowing the user to select an object from the original scene to remove</li>
        <ol style="list-style-type: lower-alpha">
            <li style="margin-left:1em">Select to remove the object that was removed for the second picture of that same scene</li>
        </ol>
    <li>Run our blob removal and replacement program</li>
    <li>Compare the result of this program to the real picture of the scene with the object removed></li>
        <ol style="list-style-type: lower-alpha">
            <li style="margin-left:1em">Compare the RGB values of all pixels in the two images to quantify how different the two images are</li>
            <li style="margin-left:1em">Qualitatively evaluate the resulting image by showing it to humans and asking them to identify any disruptions they can perceive within the image</li>
        </ol>
    <li> Modify our approach for applying the image processing algorithms by varying values such as k in k-means clustering and window size in the image quilting algorithm.</li>
</ol>

<br>
<h4>Data collection protocol</h4>
We will collect our own data by taking pictures of scenes with various objects to potentially remove and taking pictures of the same scenes with the objects physically removed. In order to ensure that the versions of the picture with and without certain objects are as similar as possible, the camera used to take the pictures will be mounted on a stationary stand and the pictures will be taken within a few seconds of each other to reduce potential for changes in lighting. Pictures will be taken with many different colored and textured backgrounds, and the objects to remove will come in many different shapes and sizes. This will provide us with a wide range of different scenarios to test the effectiveness of our object removal program.

<br><br>
<h4>Project success</h4>
Our algorithm will detect blobs in an image and display them to the user. The user will then be able to select the blobs they want to remove. At that point, the algorithm should remove the selected blobs and fill them in so that the image does not appear disrupted. Success will meet the following criteria
<br><br>Acceptance Criteria 1: When we compare the two images (one produced from our algorithm, and one real picture that has the correct object removed), we will expect 70 to 80 percent of the pixels to be within +/- 10 RGB values.
<br><br>Acceptance Criteria 2: We will also gather qualitative data for the images. We will determine if all of the selected objects were removed from the image and if all of the holes left by the objects were filled without significantly disrupting the image. We will expect at least 90% of the images to meet these criteria.
<br><br>Acceptance Criteria 3: The correct objects should be displayed to the user, so that they can select the blobs that they want to remove.

<br><br>
<h4>Outcomes</h4>
Expected: We expect our project to make improvements to basic seam carving algorithms by removing objects instead of the least important seams. Removing these objects should noticeably impact the image besides removing the object.
<br><br>Unexpected: Removing objects from images and filling in the background might yield unexpected results. There could be important portions of the image that were removed accidentally. Additionally, there could be unexpected results when we fill in the holes left by objects such as incorrect pattern matching.

  <hr>
  <footer> 
  <p>Â© Anna Dingler, Henna Mian, Allison Nakazawa, Willem Taylor, and Jessica Tepper</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
